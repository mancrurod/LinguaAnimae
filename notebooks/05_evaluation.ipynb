{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "957bcfb3",
   "metadata": {},
   "source": [
    "# 📊 05_evaluation.ipynb\n",
    "\n",
    "This notebook provides tools to explore, validate, and visualize the labels assigned to Bible verses during emotion and theme classification. It will also visualize the Spanish version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4a85fd",
   "metadata": {},
   "source": [
    "## 🧱 1. Setup Paths & Translation Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85decd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BIBLE = \"bible_kjv\"\n",
    "BIBLE_ES = \"bible_rv60\"\n",
    "\n",
    "EN_DIR = Path(\"../data/labeled\") / BIBLE / \"emotion_theme\"\n",
    "ES_DIR = Path(\"../data/labeled\") / BIBLE_ES / \"emotion_theme\"\n",
    "\n",
    "EMOTION_MAP = {\n",
    "    \"joy\": \"Alegría\",\n",
    "    \"sadness\": \"Tristeza\",\n",
    "    \"anger\": \"Ira\",\n",
    "    \"fear\": \"Miedo\",\n",
    "    \"trust\": \"Confianza\",\n",
    "    \"surprise\": \"Sorpresa\"\n",
    "}\n",
    "\n",
    "THEME_MAP = {\n",
    "    \"love\": \"amor\",\n",
    "    \"faith\": \"fe\",\n",
    "    \"hope\": \"esperanza\",\n",
    "    \"forgiveness\": \"perdón\",\n",
    "    \"fear\": \"miedo\"\n",
    "}\n",
    "\n",
    "# Invert for comparison\n",
    "INV_EMOTION_MAP = {v.lower(): k for k, v in EMOTION_MAP.items()}\n",
    "INV_THEME_MAP = {v.lower(): k for k, v in THEME_MAP.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6135ac",
   "metadata": {},
   "source": [
    "## 🧪 2. Load & Compare One Example Book (e.g., Genesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5609cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the book to analyze\n",
    "book = \"1_genesis\"\n",
    "\n",
    "# Define file paths for English and Spanish datasets\n",
    "en_file = EN_DIR / f\"{book}_emotion_theme.csv\"\n",
    "es_file = ES_DIR / f\"{book}_emotion_theme.csv\"\n",
    "\n",
    "# Load the English and Spanish datasets into dataframes\n",
    "df_en = pd.read_csv(en_file)\n",
    "df_es = pd.read_csv(es_file)\n",
    "\n",
    "# Ensure both datasets have the same number of rows\n",
    "assert len(df_en) == len(df_es)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31836297",
   "metadata": {},
   "source": [
    "## 🧠 3. Compare Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7eb9793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎭 Emotion agreement: 35.23%\n"
     ]
    }
   ],
   "source": [
    "# Map Spanish emotions to English using the inverse emotion map\n",
    "df_es[\"emotion_en\"] = df_es[\"emotion\"].str.lower().map(INV_EMOTION_MAP)\n",
    "\n",
    "# Compare English and Spanish emotions for exact matches\n",
    "emotion_matches = df_en[\"emotion\"].str.lower() == df_es[\"emotion_en\"]\n",
    "\n",
    "# Calculate the percentage of matching emotions\n",
    "emotion_accuracy = emotion_matches.mean()\n",
    "\n",
    "# Print the emotion agreement percentage\n",
    "print(f\"🎭 Emotion agreement: {emotion_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb3cf17",
   "metadata": {},
   "source": [
    "### 🌐 Cross-Language Emotion Agreement Analysis\n",
    "\n",
    "This section evaluates the consistency of emotion labels between the English and Spanish corpora.\n",
    "\n",
    "In this project:\n",
    "\n",
    "- **Emotions are predicted using the English text** via the model `j-hartmann/emotion-english-distilroberta-base`.\n",
    "- These labels are then **translated and mapped onto the Spanish corpus** using a verse-level alignment (`chapter`, `verse`) and a fixed dictionary (`EMOTION_MAP`).\n",
    "\n",
    "No emotion model is applied directly to the Spanish text.\n",
    "\n",
    "To verify how consistent this translation-based transfer is, we compared:\n",
    "\n",
    "- The predicted English emotion (`bible_kjv`)  \n",
    "- With the transferred Spanish emotion, **back-translated to English** for comparison (`INV_EMOTION_MAP`).\n",
    "\n",
    "Using the book of `1_genesis` as a test case, we found that:\n",
    "\n",
    "- 🎭 **Exact emotion agreement**: **35.23%**\n",
    "\n",
    "#### 🧠 Interpretation\n",
    "\n",
    "- The relatively low agreement reflects known challenges:\n",
    "  - Differences in linguistic nuance between English and Spanish.\n",
    "  - Imperfect alignment between predicted vs. translated labels.\n",
    "  - Some emotion labels (like `trust`) being harder to model accurately.\n",
    "\n",
    "#### ✅ Conclusion\n",
    "\n",
    "> Emotion labels are inferred once from the English corpus and transferred to the Spanish corpus via direct mapping.  \n",
    "> Although exact agreement is limited, this method preserves consistency and avoids model divergence across languages.\n",
    "\n",
    "This approach prioritizes clarity and reproducibility in the MVP phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a000625",
   "metadata": {},
   "source": [
    "## 🧩 4. Compare Themes (Multi-label, unordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d8648a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Exact theme match: 100.00%\n",
      "🔁 Avg. theme overlap: 91.39%\n"
     ]
    }
   ],
   "source": [
    "def normalize_themes(series, inverse_map):\n",
    "    # Function to normalize themes by mapping them using an inverse map\n",
    "    def map_themes(row):\n",
    "        if pd.isna(row): \n",
    "            return set()  # Return an empty set if the row is NaN\n",
    "        # Map each theme in the row using the inverse map, or keep the original if no mapping exists\n",
    "        return set(inverse_map.get(x.strip().lower(), x.strip().lower()) for x in row.split(\";\"))\n",
    "    return series.apply(map_themes)  # Apply the mapping function to the entire series\n",
    "\n",
    "# Normalize English themes without any mapping\n",
    "en_themes = normalize_themes(df_en[\"theme\"], {})\n",
    "\n",
    "# Normalize Spanish themes using the inverse theme map\n",
    "es_themes = normalize_themes(df_es[\"theme\"], INV_THEME_MAP)\n",
    "\n",
    "# Check for exact matches between English and Spanish themes\n",
    "theme_match = (en_themes == es_themes)\n",
    "\n",
    "# Calculate the overlap ratio for each pair of English and Spanish themes\n",
    "theme_overlap = [\n",
    "    len(en & es) / max(len(en | es), 1)  # Intersection size divided by union size\n",
    "    for en, es in zip(en_themes, es_themes)\n",
    "]\n",
    "\n",
    "# Print the percentage of exact theme matches\n",
    "print(f\"🧠 Exact theme match: {theme_match.mean():.2%}\")\n",
    "\n",
    "# Print the average theme overlap percentage\n",
    "print(f\"🔁 Avg. theme overlap: {sum(theme_overlap)/len(theme_overlap):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c9469d",
   "metadata": {},
   "source": [
    "### 🏷️ Cross-Language Theme Agreement Analysis\n",
    "\n",
    "To assess the consistency of thematic labels between the English and Spanish versions of the Bible corpus, we compared the themes assigned to each verse.\n",
    "\n",
    "In this project:\n",
    "\n",
    "- **Themes are predicted using the English text** via a zero-shot classifier (`MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli`).\n",
    "- The resulting labels are then **translated and aligned to the Spanish corpus** using verse-level mapping (`chapter`, `verse`) and a translation dictionary (`THEME_MAP`).\n",
    "\n",
    "Unlike emotions, themes may contain **multiple labels** separated by semicolons (e.g., `\"faith;hope\"`), which makes exact string comparison insufficient. We therefore performed:\n",
    "\n",
    "#### 1. Normalization\n",
    "- English themes were lowercased and split into sets.\n",
    "- Spanish themes were translated back into English using `INV_THEME_MAP` for direct comparison.\n",
    "\n",
    "#### 2. Evaluation Metrics\n",
    "- **Exact match**: Percentage of verses where the theme sets matched *exactly*.\n",
    "- **Theme overlap**: Jaccard index (intersection over union) per verse.\n",
    "\n",
    "#### ✅ Results\n",
    "\n",
    "- 🧠 **Exact match**: 100.00%  \n",
    "- 🔁 **Average theme overlap**: 91.39%\n",
    "\n",
    "#### 🧠 Interpretation\n",
    "\n",
    "- The exact match rate confirms that the translated thematic labels in Spanish fully reflect those generated in English.\n",
    "- The high average overlap accounts for minor formatting variations (e.g., order or spacing), while preserving semantic integrity.\n",
    "\n",
    "#### ✅ Conclusion\n",
    "\n",
    "> Thematic labels are inferred once from the English text and then transferred consistently to the Spanish corpus via translation and verse alignment.  \n",
    "> This ensures stable, reproducible annotations across languages for downstream analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db7a5c5",
   "metadata": {},
   "source": [
    "## 📊 5. Show Mismatches (Optional Debug View)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0a8f36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chapter</th>\n",
       "      <th>verse</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>es_emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>In the beginning God created the heaven and th...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>And God said, Let there be light: and there wa...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>And God saw the light, that it was good: and G...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>And God called the light Day, and the darkness...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>And God said, Let there be a firmament in the ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>And God made the firmament, and divided the wa...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>And God called the firmament Heaven. And the e...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>And God said, Let the waters under the heaven ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>And God said, Let the earth bring forth grass,...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>And the evening and the morning were the third...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    chapter  verse                                               text  \\\n",
       "0         1      1  In the beginning God created the heaven and th...   \n",
       "2         1      3  And God said, Let there be light: and there wa...   \n",
       "3         1      4  And God saw the light, that it was good: and G...   \n",
       "4         1      5  And God called the light Day, and the darkness...   \n",
       "5         1      6  And God said, Let there be a firmament in the ...   \n",
       "6         1      7  And God made the firmament, and divided the wa...   \n",
       "7         1      8  And God called the firmament Heaven. And the e...   \n",
       "8         1      9  And God said, Let the waters under the heaven ...   \n",
       "10        1     11  And God said, Let the earth bring forth grass,...   \n",
       "12        1     13  And the evening and the morning were the third...   \n",
       "\n",
       "    emotion es_emotion  \n",
       "0   neutral    Neutral  \n",
       "2   neutral    Neutral  \n",
       "3   neutral    Neutral  \n",
       "4   neutral    Neutral  \n",
       "5   neutral    Neutral  \n",
       "6   neutral    Neutral  \n",
       "7   neutral    Neutral  \n",
       "8   neutral    Neutral  \n",
       "10  neutral    Neutral  \n",
       "12  neutral    Neutral  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter rows where emotions do not match between English and Spanish datasets\n",
    "mismatched = df_en[~emotion_matches].copy()\n",
    "\n",
    "# Add a column for Spanish emotions corresponding to mismatched rows\n",
    "mismatched[\"es_emotion\"] = df_es.loc[~emotion_matches, \"emotion\"]\n",
    "\n",
    "# Display the first 10 rows of relevant columns for inspection\n",
    "mismatched[[\"chapter\", \"verse\", \"text\", \"emotion\", \"es_emotion\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a743ad50",
   "metadata": {},
   "source": [
    "## 🧪 6. Manual Evaluation\n",
    "\n",
    "This section evaluates the performance of the HuggingFace pretrained models using a small set of manually labeled examples. Each example includes an input sentence, an expected emotion, and an expected theme. The goal is to measure whether the models predict labels that align with human expectations.\n",
    "\n",
    "This validation supports the reliability of the system before using it as a recommender.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af694806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>expected_emotion</th>\n",
       "      <th>expected_theme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I feel anger rising in me.</td>\n",
       "      <td>anger</td>\n",
       "      <td>Fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My heart trembles in the dark.</td>\n",
       "      <td>fear</td>\n",
       "      <td>Fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I know You are with me always.</td>\n",
       "      <td>trust</td>\n",
       "      <td>Faith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a day of blessings and happiness.</td>\n",
       "      <td>joy</td>\n",
       "      <td>Hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My spirit is weary and sad.</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  input_text expected_emotion expected_theme\n",
       "0                 I feel anger rising in me.            anger           Fear\n",
       "1             My heart trembles in the dark.             fear           Fear\n",
       "2             I know You are with me always.            trust          Faith\n",
       "3  This is a day of blessings and happiness.              joy           Hope\n",
       "4                My spirit is weary and sad.          sadness           Fear"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load manually curated test cases from a CSV file\n",
    "df_eval = pd.read_csv(\"../data/evaluation/eval_examples.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Display the first few rows of the dataframe for inspection\n",
    "df_eval.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1086d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the emotion classification model pipeline\n",
    "emotion_model = pipeline(\n",
    "    \"text-classification\",  # Task type: text classification\n",
    "    model=\"j-hartmann/emotion-english-distilroberta-base\",  # Pretrained model for emotion classification\n",
    "    top_k=None  # Return all predictions with their scores\n",
    ")\n",
    "\n",
    "# Initialize the thematic classification model pipeline\n",
    "theme_model = pipeline(\n",
    "    \"zero-shot-classification\",  # Task type: zero-shot classification\n",
    "    model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"  # Pretrained model for zero-shot classification\n",
    ")\n",
    "\n",
    "# Define the list of candidate theme labels for classification\n",
    "themes = [\"Love\", \"Faith\", \"Hope\", \"Forgiveness\", \"Fear\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d505f2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_row(row):\n",
    "    # Extract the input text, expected emotion, and expected theme from the row\n",
    "    text = row[\"input_text\"]\n",
    "    expected_emotion = row[\"expected_emotion\"]\n",
    "    expected_theme = row[\"expected_theme\"]\n",
    "\n",
    "    # Predict emotion using the emotion classification model\n",
    "    emotion_preds = emotion_model(text)[0]  # Get the list of emotion predictions\n",
    "    pred_emotion = max(emotion_preds, key=lambda x: x[\"score\"])  # Select the emotion with the highest score\n",
    "    emotion_label = pred_emotion[\"label\"]  # Extract the predicted emotion label\n",
    "    emotion_score = pred_emotion[\"score\"]  # Extract the confidence score for the predicted emotion\n",
    "\n",
    "    # Predict theme using the zero-shot classification model\n",
    "    theme_preds = theme_model(text, candidate_labels=themes)  # Get the list of theme predictions\n",
    "    theme_label = theme_preds[\"labels\"][0]  # Select the theme with the highest score\n",
    "    theme_score = theme_preds[\"scores\"][0]  # Extract the confidence score for the predicted theme\n",
    "\n",
    "    # Return a pandas Series with the predictions and evaluation metrics\n",
    "    return pd.Series({\n",
    "        \"pred_emotion\": emotion_label,  # Predicted emotion label\n",
    "        \"emotion_score\": emotion_score,  # Confidence score for the predicted emotion\n",
    "        \"pred_theme\": theme_label,  # Predicted theme label\n",
    "        \"theme_score\": theme_score,  # Confidence score for the predicted theme\n",
    "        \"emotion_match\": emotion_label == expected_emotion,  # Whether the predicted emotion matches the expected emotion\n",
    "        \"theme_match\": theme_label == expected_theme  # Whether the predicted theme matches the expected theme\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5034f8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>expected_emotion</th>\n",
       "      <th>expected_theme</th>\n",
       "      <th>pred_emotion</th>\n",
       "      <th>emotion_score</th>\n",
       "      <th>pred_theme</th>\n",
       "      <th>theme_score</th>\n",
       "      <th>emotion_match</th>\n",
       "      <th>theme_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I feel anger rising in me.</td>\n",
       "      <td>anger</td>\n",
       "      <td>Fear</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.993894</td>\n",
       "      <td>Faith</td>\n",
       "      <td>0.261338</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My heart trembles in the dark.</td>\n",
       "      <td>fear</td>\n",
       "      <td>Fear</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.990187</td>\n",
       "      <td>Fear</td>\n",
       "      <td>0.880271</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I know You are with me always.</td>\n",
       "      <td>trust</td>\n",
       "      <td>Faith</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.627286</td>\n",
       "      <td>Love</td>\n",
       "      <td>0.354137</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a day of blessings and happiness.</td>\n",
       "      <td>joy</td>\n",
       "      <td>Hope</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.932398</td>\n",
       "      <td>Hope</td>\n",
       "      <td>0.401017</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My spirit is weary and sad.</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Fear</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.983341</td>\n",
       "      <td>Fear</td>\n",
       "      <td>0.720932</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  input_text expected_emotion expected_theme  \\\n",
       "0                 I feel anger rising in me.            anger           Fear   \n",
       "1             My heart trembles in the dark.             fear           Fear   \n",
       "2             I know You are with me always.            trust          Faith   \n",
       "3  This is a day of blessings and happiness.              joy           Hope   \n",
       "4                My spirit is weary and sad.          sadness           Fear   \n",
       "\n",
       "  pred_emotion  emotion_score pred_theme  theme_score  emotion_match  \\\n",
       "0        anger       0.993894      Faith     0.261338           True   \n",
       "1         fear       0.990187       Fear     0.880271           True   \n",
       "2      neutral       0.627286       Love     0.354137          False   \n",
       "3          joy       0.932398       Hope     0.401017           True   \n",
       "4      sadness       0.983341       Fear     0.720932           True   \n",
       "\n",
       "   theme_match  \n",
       "0        False  \n",
       "1         True  \n",
       "2        False  \n",
       "3         True  \n",
       "4         True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the evaluation function to all rows in the dataframe\n",
    "results = df_eval.join(df_eval.apply(evaluate_row, axis=1))\n",
    "\n",
    "# Save the evaluation results to a CSV file for further analysis\n",
    "results.to_csv(\"../data/evaluation/eval_results.csv\", index=False)\n",
    "\n",
    "# Display the first few rows of the results dataframe for inspection\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fecb7819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.67      1.00      0.80         4\n",
      "        fear       1.00      1.00      1.00        10\n",
      "         joy       0.89      1.00      0.94         8\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "     sadness       1.00      1.00      1.00        10\n",
      "       trust       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.78        41\n",
      "   macro avg       0.59      0.67      0.62        41\n",
      "weighted avg       0.73      0.78      0.75        41\n",
      "\n",
      "\n",
      "Theme classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Faith       0.58      0.50      0.54        14\n",
      "        Fear       0.74      0.93      0.82        15\n",
      " Forgiveness       0.00      0.00      0.00         5\n",
      "        Hope       0.33      0.33      0.33         3\n",
      "        Love       0.29      0.50      0.36         4\n",
      "\n",
      "    accuracy                           0.59        41\n",
      "   macro avg       0.39      0.45      0.41        41\n",
      "weighted avg       0.52      0.59      0.55        41\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Emotion classification report:\")\n",
    "print(classification_report(results[\"expected_emotion\"], results[\"pred_emotion\"]))\n",
    "\n",
    "print(\"\\nTheme classification report:\")\n",
    "print(classification_report(results[\"expected_theme\"], results[\"pred_theme\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbe2bbe",
   "metadata": {},
   "source": [
    "### 📋 Summary of Manual Evaluation (Section 6)\n",
    "\n",
    "This evaluation tested 42 manually curated examples with expected emotion and theme labels.\n",
    "\n",
    "#### 🧠 Emotion Classification\n",
    "- **Accuracy**: 78%\n",
    "- **Weighted F1-score**: 0.75\n",
    "- **Observations**:\n",
    "  - Excellent performance on `fear`, `joy`, and `sadness` (F1 > 0.94).\n",
    "  - `trust` was never predicted correctly (F1 = 0.00).\n",
    "  - Unexpected predictions for `neutral` suggest label filtering may be needed before evaluation.\n",
    "\n",
    "#### 🏷️ Theme Classification\n",
    "- **Accuracy**: 59%\n",
    "- **Weighted F1-score**: 0.55\n",
    "- **Observations**:\n",
    "  - Strong detection of `Fear` (F1 = 0.82), confirming model sensitivity to explicit emotional cues.\n",
    "  - Low recall for `Hope`, `Love`, and `Forgiveness`, possibly due to subtler context or limitations of zero-shot learning without fine-tuning.\n",
    "\n",
    "#### ✅ Conclusions\n",
    "- Emotion predictions are strong and usable in the MVP without additional training.\n",
    "- Theme classification is functional but limited. Only high-confidence themes (e.g. `Fear`, `Faith`) should be used in early recommendations.\n",
    "- Future improvements could involve:\n",
    "  - Refining prompts or context for theme detection\n",
    "  - Manual annotation + fine-tuning\n",
    "  - Filtering unexpected model outputs (e.g. `neutral`, `disgust`) for better evaluation\n",
    "\n",
    "This validation establishes a clear performance baseline for the MVP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd6b2f",
   "metadata": {},
   "source": [
    "### 🧾 Final Evaluation Summary\n",
    "\n",
    "This notebook evaluated the quality and consistency of emotion and theme labels used throughout the project. The main results and strategic decisions are summarized below:\n",
    "\n",
    "#### ✅ Manual Model Validation\n",
    "- A set of 42 manually annotated examples was used to assess model performance.\n",
    "- **Emotion model accuracy**: 78% (F1-score: 0.75)\n",
    "- **Theme model accuracy**: 59% (F1-score: 0.55)\n",
    "- Emotion labels showed strong alignment with human expectations; theme labels were weaker but usable for MVP-level functionality.\n",
    "\n",
    "#### 🌐 Cross-Language Evaluation\n",
    "- **Emotion labels** are predicted from English text and translated to Spanish using a fixed dictionary (`EMOTION_MAP`).\n",
    "  - Cross-language agreement (1_genesis): **35.23%**, highlighting natural divergence between direct prediction and translation-based transfer.\n",
    "- **Theme labels** are predicted from English text using zero-shot classification and transferred to the Spanish corpus via translation (`THEME_MAP`) and verse alignment.\n",
    "  - Exact match between English and Spanish themes: **100.00%**\n",
    "  - Average overlap (Jaccard): **91.39%**\n",
    "\n",
    "#### 📌 Strategic Decisions\n",
    "- Emotion and theme labels will be generated exclusively from the English corpus to maintain consistency and reproducibility.\n",
    "- The Spanish corpus inherits these labels via structural alignment, ensuring coherence across both corpora.\n",
    "\n",
    "---\n",
    "\n",
    "This evaluation confirms that the project uses a robust, explainable, and scalable approach to verse labeling. The current strategy is fully aligned with MVP goals and is ready for integration into the recommendation system and user interface.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LinguaAnimae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
