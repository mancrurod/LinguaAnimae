{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "957bcfb3",
   "metadata": {},
   "source": [
    "# üìä 05_evaluation.ipynb\n",
    "\n",
    "This notebook provides tools to explore, validate, and visualize the labels assigned to Bible verses during emotion and theme classification. It will also visualize the Spanish version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4a85fd",
   "metadata": {},
   "source": [
    "## üß± 1. Setup Paths & Translation Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85decd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BIBLE = \"bible_kjv\"\n",
    "BIBLE_ES = \"bible_rv60\"\n",
    "\n",
    "EN_DIR = Path(\"data/labeled\") / BIBLE / \"emotion_theme\"\n",
    "ES_DIR = Path(\"data/labeled\") / BIBLE_ES / \"emotion_theme\"\n",
    "\n",
    "EMOTION_MAP = {\n",
    "    \"joy\": \"Alegr√≠a\",\n",
    "    \"sadness\": \"Tristeza\",\n",
    "    \"anger\": \"Ira\",\n",
    "    \"fear\": \"Miedo\",\n",
    "    \"trust\": \"Confianza\",\n",
    "    \"surprise\": \"Sorpresa\"\n",
    "}\n",
    "\n",
    "THEME_MAP = {\n",
    "    \"love\": \"amor\",\n",
    "    \"faith\": \"fe\",\n",
    "    \"hope\": \"esperanza\",\n",
    "    \"forgiveness\": \"perd√≥n\",\n",
    "    \"fear\": \"miedo\"\n",
    "}\n",
    "\n",
    "# Invert for comparison\n",
    "INV_EMOTION_MAP = {v.lower(): k for k, v in EMOTION_MAP.items()}\n",
    "INV_THEME_MAP = {v.lower(): k for k, v in THEME_MAP.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6135ac",
   "metadata": {},
   "source": [
    "## üß™ 2. Load & Compare One Example Book (e.g., Genesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5609cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "book = \"1_genesis\"\n",
    "en_file = EN_DIR / f\"{book}_emotion_theme.csv\"\n",
    "es_file = ES_DIR / f\"{book}_emotion_theme.csv\"\n",
    "\n",
    "df_en = pd.read_csv(en_file)\n",
    "df_es = pd.read_csv(es_file)\n",
    "\n",
    "assert len(df_en) == len(df_es)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31836297",
   "metadata": {},
   "source": [
    "## üß† 3. Compare Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eb9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Spanish ‚Üí English\n",
    "df_es[\"emotion_en\"] = df_es[\"emotion\"].str.lower().map(INV_EMOTION_MAP)\n",
    "emotion_matches = df_en[\"emotion\"].str.lower() == df_es[\"emotion_en\"]\n",
    "emotion_accuracy = emotion_matches.mean()\n",
    "\n",
    "print(f\"üé≠ Emotion agreement: {emotion_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a000625",
   "metadata": {},
   "source": [
    "## üß© 4. Compare Themes (Multi-label, unordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8648a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_themes(series, inverse_map):\n",
    "    def map_themes(row):\n",
    "        if pd.isna(row): return set()\n",
    "        return set(inverse_map.get(x.strip().lower(), x.strip().lower()) for x in row.split(\";\"))\n",
    "    return series.apply(map_themes)\n",
    "\n",
    "en_themes = normalize_themes(df_en[\"theme\"], {})\n",
    "es_themes = normalize_themes(df_es[\"theme\"], INV_THEME_MAP)\n",
    "\n",
    "theme_match = (en_themes == es_themes)\n",
    "theme_overlap = [\n",
    "    len(en & es) / max(len(en | es), 1)\n",
    "    for en, es in zip(en_themes, es_themes)\n",
    "]\n",
    "\n",
    "print(f\"üß† Exact theme match: {theme_match.mean():.2%}\")\n",
    "print(f\"üîÅ Avg. theme overlap: {sum(theme_overlap)/len(theme_overlap):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db7a5c5",
   "metadata": {},
   "source": [
    "## üìä 5. Show Mismatches (Optional Debug View)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a8f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatched = df_en[~emotion_matches].copy()\n",
    "mismatched[\"es_emotion\"] = df_es.loc[~emotion_matches, \"emotion\"]\n",
    "mismatched[[\"chapter\", \"verse\", \"text\", \"emotion\", \"es_emotion\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a743ad50",
   "metadata": {},
   "source": [
    "## üß™ 6. Manual Evaluation\n",
    "\n",
    "This section evaluates the performance of the HuggingFace pretrained models using a small set of manually labeled examples. Each example includes an input sentence, an expected emotion, and an expected theme. The goal is to measure whether the models predict labels that align with human expectations.\n",
    "\n",
    "This validation supports the reliability of the system before using it as a recommender.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af694806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load manually curated test cases\n",
    "df_eval = pd.read_csv(\"data/evaluation/eval_examples.csv\")\n",
    "df_eval.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1086d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Emotion classification model\n",
    "emotion_model = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "    top_k=None\n",
    ")\n",
    "\n",
    "# Thematic classification model\n",
    "theme_model = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    ")\n",
    "\n",
    "# Candidate theme labels\n",
    "themes = [\"Love\", \"Faith\", \"Hope\", \"Forgiveness\", \"Fear\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d505f2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_row(row):\n",
    "    text = row[\"input_text\"]\n",
    "    expected_emotion = row[\"expected_emotion\"]\n",
    "    expected_theme = row[\"expected_theme\"]\n",
    "\n",
    "    # Predict emotion\n",
    "    pred_emotion = max(emotion_model(text)[0], key=lambda x: x[\"score\"])[\"label\"]\n",
    "\n",
    "    # Predict theme\n",
    "    pred_theme = theme_model(text, candidate_labels=themes)[\"labels\"][0]\n",
    "\n",
    "    return pd.Series({\n",
    "        \"pred_emotion\": pred_emotion,\n",
    "        \"pred_theme\": pred_theme,\n",
    "        \"emotion_match\": pred_emotion == expected_emotion,\n",
    "        \"theme_match\": pred_theme == expected_theme\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5034f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply evaluation to all rows\n",
    "results = df_eval.join(df_eval.apply(evaluate_row, axis=1))\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb7819",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_accuracy = results[\"emotion_match\"].mean()\n",
    "theme_accuracy = results[\"theme_match\"].mean()\n",
    "\n",
    "print(f\"üéØ Emotion accuracy: {emotion_accuracy:.2%}\")\n",
    "print(f\"üè∑Ô∏è Theme accuracy: {theme_accuracy:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
