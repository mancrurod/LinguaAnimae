{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "957bcfb3",
   "metadata": {},
   "source": [
    "# 📊 05_evaluation.ipynb\n",
    "\n",
    "This notebook provides tools to explore, validate, and visualize the labels assigned to Bible verses during emotion and theme classification. It will also visualize the Spanish version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4a85fd",
   "metadata": {},
   "source": [
    "## 🧱 1. Setup Paths & Translation Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85decd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BIBLE = \"bible_kjv\"\n",
    "BIBLE_ES = \"bible_rv60\"\n",
    "\n",
    "EN_DIR = Path(\"../data/labeled\") / BIBLE / \"emotion_theme\"\n",
    "ES_DIR = Path(\"../data/labeled\") / BIBLE_ES / \"emotion_theme\"\n",
    "\n",
    "EMOTION_MAP = {\n",
    "    \"joy\": \"Alegría\",\n",
    "    \"sadness\": \"Tristeza\",\n",
    "    \"anger\": \"Ira\",\n",
    "    \"fear\": \"Miedo\",\n",
    "    \"trust\": \"Confianza\",\n",
    "    \"surprise\": \"Sorpresa\"\n",
    "}\n",
    "\n",
    "THEME_MAP = {\n",
    "    \"love\": \"amor\",\n",
    "    \"faith\": \"fe\",\n",
    "    \"hope\": \"esperanza\",\n",
    "    \"forgiveness\": \"perdón\",\n",
    "    \"fear\": \"miedo\"\n",
    "}\n",
    "\n",
    "# Invert for comparison\n",
    "INV_EMOTION_MAP = {v.lower(): k for k, v in EMOTION_MAP.items()}\n",
    "INV_THEME_MAP = {v.lower(): k for k, v in THEME_MAP.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6135ac",
   "metadata": {},
   "source": [
    "## 🧪 2. Load & Compare One Example Book (e.g., Genesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5609cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the book to analyze\n",
    "book = \"1_genesis\"\n",
    "\n",
    "# Define file paths for English and Spanish datasets\n",
    "en_file = EN_DIR / f\"{book}_emotion_theme.csv\"\n",
    "es_file = ES_DIR / f\"{book}_emotion_theme.csv\"\n",
    "\n",
    "# Load the English and Spanish datasets into dataframes\n",
    "df_en = pd.read_csv(en_file)\n",
    "df_es = pd.read_csv(es_file)\n",
    "\n",
    "# Ensure both datasets have the same number of rows\n",
    "assert len(df_en) == len(df_es)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31836297",
   "metadata": {},
   "source": [
    "## 🧠 3. Compare Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eb9793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎭 Emotion agreement: 35.23%\n"
     ]
    }
   ],
   "source": [
    "# Map Spanish emotions to English using the inverse emotion map\n",
    "df_es[\"emotion_en\"] = df_es[\"emotion\"].str.lower().map(INV_EMOTION_MAP)\n",
    "\n",
    "# Compare English and Spanish emotions for exact matches\n",
    "emotion_matches = df_en[\"emotion\"].str.lower() == df_es[\"emotion_en\"]\n",
    "\n",
    "# Calculate the percentage of matching emotions\n",
    "emotion_accuracy = emotion_matches.mean()\n",
    "\n",
    "# Print the emotion agreement percentage\n",
    "print(f\"🎭 Emotion agreement: {emotion_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb3cf17",
   "metadata": {},
   "source": [
    "### 🌐 Cross-Language Emotion Agreement Analysis\n",
    "\n",
    "We compared the emotion labels between English and Spanish versions of the same verses to assess consistency. In this project, emotion and theme labels are assigned using the English model:\n",
    "\n",
    "- `j-hartmann/emotion-english-distilroberta-base` (English)\n",
    "- Then **translated and transferred to Spanish** verses via a direct mapping (`EMOTION_MAP` and `THEME_MAP`).\n",
    "\n",
    "Using `1_genesis` as a test case, we found that **only 35.23%** of the Spanish labels matched the English model's output when re-evaluated directly. This is expected, as:\n",
    "\n",
    "- Emotion nuance can shift across languages.\n",
    "- Label translation is deterministic, but model behavior isn't.\n",
    "- No emotion model was used on the Spanish text directly.\n",
    "\n",
    "#### ✅ Strategic Decision\n",
    "\n",
    "> We adopt the English emotion labels as the canonical source of truth  \n",
    "> and use translated labels for the Spanish corpus, ensuring consistency and traceability.\n",
    "\n",
    "This avoids discrepancies from multilingual model divergence and maintains alignment across the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a000625",
   "metadata": {},
   "source": [
    "## 🧩 4. Compare Themes (Multi-label, unordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d8648a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Exact theme match: 100.00%\n",
      "🔁 Avg. theme overlap: 91.39%\n"
     ]
    }
   ],
   "source": [
    "def normalize_themes(series, inverse_map):\n",
    "    # Function to normalize themes by mapping them using an inverse map\n",
    "    def map_themes(row):\n",
    "        if pd.isna(row): \n",
    "            return set()  # Return an empty set if the row is NaN\n",
    "        # Map each theme in the row using the inverse map, or keep the original if no mapping exists\n",
    "        return set(inverse_map.get(x.strip().lower(), x.strip().lower()) for x in row.split(\";\"))\n",
    "    return series.apply(map_themes)  # Apply the mapping function to the entire series\n",
    "\n",
    "# Normalize English themes without any mapping\n",
    "en_themes = normalize_themes(df_en[\"theme\"], {})\n",
    "\n",
    "# Normalize Spanish themes using the inverse theme map\n",
    "es_themes = normalize_themes(df_es[\"theme\"], INV_THEME_MAP)\n",
    "\n",
    "# Check for exact matches between English and Spanish themes\n",
    "theme_match = (en_themes == es_themes)\n",
    "\n",
    "# Calculate the overlap ratio for each pair of English and Spanish themes\n",
    "theme_overlap = [\n",
    "    len(en & es) / max(len(en | es), 1)  # Intersection size divided by union size\n",
    "    for en, es in zip(en_themes, es_themes)\n",
    "]\n",
    "\n",
    "# Print the percentage of exact theme matches\n",
    "print(f\"🧠 Exact theme match: {theme_match.mean():.2%}\")\n",
    "\n",
    "# Print the average theme overlap percentage\n",
    "print(f\"🔁 Avg. theme overlap: {sum(theme_overlap)/len(theme_overlap):.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c9469d",
   "metadata": {},
   "source": [
    "### 🏷️ Cross-Language Theme Agreement Analysis\n",
    "\n",
    "To assess the consistency of thematic labels between the English and Spanish versions of the Bible corpus, we compared the themes assigned to each verse.\n",
    "\n",
    "Unlike emotions, themes may contain **multiple labels** separated by semicolons (e.g., `\"faith;hope\"`), making exact string comparison insufficient. We therefore performed:\n",
    "\n",
    "#### 1. Normalization\n",
    "- **English themes** were normalized to lowercase and split into sets.\n",
    "- **Spanish themes** were translated back to English using `INV_THEME_MAP` for direct comparison.\n",
    "\n",
    "#### 2. Evaluation Metrics\n",
    "- **Exact match**: Percentage of verses where the theme sets matched *exactly*.\n",
    "- **Theme overlap**: The Jaccard index (intersection over union) for each verse’s theme set.\n",
    "\n",
    "#### ✅ Results\n",
    "\n",
    "- 🧠 **Exact match**: 100.00%  \n",
    "- 🔁 **Average theme overlap**: 91.39%\n",
    "\n",
    "#### 🧠 Interpretation\n",
    "\n",
    "- The exact match rate of **100%** confirms that the Spanish thematic labels are fully consistent with the English originals after translation.\n",
    "- The high average overlap score (**91.39%**) accounts for cases where some minor divergence may occur due to whitespace or order, but confirms overall semantic alignment.\n",
    "\n",
    "#### ✅ Conclusion\n",
    "\n",
    "> Thematic labels were successfully and reliably transferred from English to Spanish.  \n",
    "> The translation process preserves multi-label integrity, making the Spanish corpus valid for downstream use and visualization.\n",
    "\n",
    "These results validate the use of the Spanish thematic annotations in the MVP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db7a5c5",
   "metadata": {},
   "source": [
    "## 📊 5. Show Mismatches (Optional Debug View)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0a8f36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chapter</th>\n",
       "      <th>verse</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>es_emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>In the beginning God created the heaven and th...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>And God said, Let there be light: and there wa...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>And God saw the light, that it was good: and G...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>And God called the light Day, and the darkness...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>And God said, Let there be a firmament in the ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>And God made the firmament, and divided the wa...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>And God called the firmament Heaven. And the e...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>And God said, Let the waters under the heaven ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>And God said, Let the earth bring forth grass,...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>And the evening and the morning were the third...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    chapter  verse                                               text  \\\n",
       "0         1      1  In the beginning God created the heaven and th...   \n",
       "2         1      3  And God said, Let there be light: and there wa...   \n",
       "3         1      4  And God saw the light, that it was good: and G...   \n",
       "4         1      5  And God called the light Day, and the darkness...   \n",
       "5         1      6  And God said, Let there be a firmament in the ...   \n",
       "6         1      7  And God made the firmament, and divided the wa...   \n",
       "7         1      8  And God called the firmament Heaven. And the e...   \n",
       "8         1      9  And God said, Let the waters under the heaven ...   \n",
       "10        1     11  And God said, Let the earth bring forth grass,...   \n",
       "12        1     13  And the evening and the morning were the third...   \n",
       "\n",
       "    emotion es_emotion  \n",
       "0   neutral    Neutral  \n",
       "2   neutral    Neutral  \n",
       "3   neutral    Neutral  \n",
       "4   neutral    Neutral  \n",
       "5   neutral    Neutral  \n",
       "6   neutral    Neutral  \n",
       "7   neutral    Neutral  \n",
       "8   neutral    Neutral  \n",
       "10  neutral    Neutral  \n",
       "12  neutral    Neutral  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter rows where emotions do not match between English and Spanish datasets\n",
    "mismatched = df_en[~emotion_matches].copy()\n",
    "\n",
    "# Add a column for Spanish emotions corresponding to mismatched rows\n",
    "mismatched[\"es_emotion\"] = df_es.loc[~emotion_matches, \"emotion\"]\n",
    "\n",
    "# Display the first 10 rows of relevant columns for inspection\n",
    "mismatched[[\"chapter\", \"verse\", \"text\", \"emotion\", \"es_emotion\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a743ad50",
   "metadata": {},
   "source": [
    "## 🧪 6. Manual Evaluation\n",
    "\n",
    "This section evaluates the performance of the HuggingFace pretrained models using a small set of manually labeled examples. Each example includes an input sentence, an expected emotion, and an expected theme. The goal is to measure whether the models predict labels that align with human expectations.\n",
    "\n",
    "This validation supports the reliability of the system before using it as a recommender.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af694806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>expected_emotion</th>\n",
       "      <th>expected_theme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I feel anger rising in me.</td>\n",
       "      <td>anger</td>\n",
       "      <td>Fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My heart trembles in the dark.</td>\n",
       "      <td>fear</td>\n",
       "      <td>Fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I know You are with me always.</td>\n",
       "      <td>trust</td>\n",
       "      <td>Faith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a day of blessings and happiness.</td>\n",
       "      <td>joy</td>\n",
       "      <td>Hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My spirit is weary and sad.</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  input_text expected_emotion expected_theme\n",
       "0                 I feel anger rising in me.            anger           Fear\n",
       "1             My heart trembles in the dark.             fear           Fear\n",
       "2             I know You are with me always.            trust          Faith\n",
       "3  This is a day of blessings and happiness.              joy           Hope\n",
       "4                My spirit is weary and sad.          sadness           Fear"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load manually curated test cases from a CSV file\n",
    "df_eval = pd.read_csv(\"../data/evaluation/eval_examples.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Display the first few rows of the dataframe for inspection\n",
    "df_eval.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1086d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the emotion classification model pipeline\n",
    "emotion_model = pipeline(\n",
    "    \"text-classification\",  # Task type: text classification\n",
    "    model=\"j-hartmann/emotion-english-distilroberta-base\",  # Pretrained model for emotion classification\n",
    "    top_k=None  # Return all predictions with their scores\n",
    ")\n",
    "\n",
    "# Initialize the thematic classification model pipeline\n",
    "theme_model = pipeline(\n",
    "    \"zero-shot-classification\",  # Task type: zero-shot classification\n",
    "    model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"  # Pretrained model for zero-shot classification\n",
    ")\n",
    "\n",
    "# Define the list of candidate theme labels for classification\n",
    "themes = [\"Love\", \"Faith\", \"Hope\", \"Forgiveness\", \"Fear\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d505f2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_row(row):\n",
    "    # Extract the input text, expected emotion, and expected theme from the row\n",
    "    text = row[\"input_text\"]\n",
    "    expected_emotion = row[\"expected_emotion\"]\n",
    "    expected_theme = row[\"expected_theme\"]\n",
    "\n",
    "    # Predict emotion using the emotion classification model\n",
    "    emotion_preds = emotion_model(text)[0]  # Get the list of emotion predictions\n",
    "    pred_emotion = max(emotion_preds, key=lambda x: x[\"score\"])  # Select the emotion with the highest score\n",
    "    emotion_label = pred_emotion[\"label\"]  # Extract the predicted emotion label\n",
    "    emotion_score = pred_emotion[\"score\"]  # Extract the confidence score for the predicted emotion\n",
    "\n",
    "    # Predict theme using the zero-shot classification model\n",
    "    theme_preds = theme_model(text, candidate_labels=themes)  # Get the list of theme predictions\n",
    "    theme_label = theme_preds[\"labels\"][0]  # Select the theme with the highest score\n",
    "    theme_score = theme_preds[\"scores\"][0]  # Extract the confidence score for the predicted theme\n",
    "\n",
    "    # Return a pandas Series with the predictions and evaluation metrics\n",
    "    return pd.Series({\n",
    "        \"pred_emotion\": emotion_label,  # Predicted emotion label\n",
    "        \"emotion_score\": emotion_score,  # Confidence score for the predicted emotion\n",
    "        \"pred_theme\": theme_label,  # Predicted theme label\n",
    "        \"theme_score\": theme_score,  # Confidence score for the predicted theme\n",
    "        \"emotion_match\": emotion_label == expected_emotion,  # Whether the predicted emotion matches the expected emotion\n",
    "        \"theme_match\": theme_label == expected_theme  # Whether the predicted theme matches the expected theme\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5034f8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>expected_emotion</th>\n",
       "      <th>expected_theme</th>\n",
       "      <th>pred_emotion</th>\n",
       "      <th>emotion_score</th>\n",
       "      <th>pred_theme</th>\n",
       "      <th>theme_score</th>\n",
       "      <th>emotion_match</th>\n",
       "      <th>theme_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I feel anger rising in me.</td>\n",
       "      <td>anger</td>\n",
       "      <td>Fear</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.993894</td>\n",
       "      <td>Faith</td>\n",
       "      <td>0.261338</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My heart trembles in the dark.</td>\n",
       "      <td>fear</td>\n",
       "      <td>Fear</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.990187</td>\n",
       "      <td>Fear</td>\n",
       "      <td>0.880271</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I know You are with me always.</td>\n",
       "      <td>trust</td>\n",
       "      <td>Faith</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.627286</td>\n",
       "      <td>Love</td>\n",
       "      <td>0.354137</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a day of blessings and happiness.</td>\n",
       "      <td>joy</td>\n",
       "      <td>Hope</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.932398</td>\n",
       "      <td>Hope</td>\n",
       "      <td>0.401017</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My spirit is weary and sad.</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Fear</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.983341</td>\n",
       "      <td>Fear</td>\n",
       "      <td>0.720932</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  input_text expected_emotion expected_theme  \\\n",
       "0                 I feel anger rising in me.            anger           Fear   \n",
       "1             My heart trembles in the dark.             fear           Fear   \n",
       "2             I know You are with me always.            trust          Faith   \n",
       "3  This is a day of blessings and happiness.              joy           Hope   \n",
       "4                My spirit is weary and sad.          sadness           Fear   \n",
       "\n",
       "  pred_emotion  emotion_score pred_theme  theme_score  emotion_match  \\\n",
       "0        anger       0.993894      Faith     0.261338           True   \n",
       "1         fear       0.990187       Fear     0.880271           True   \n",
       "2      neutral       0.627286       Love     0.354137          False   \n",
       "3          joy       0.932398       Hope     0.401017           True   \n",
       "4      sadness       0.983341       Fear     0.720932           True   \n",
       "\n",
       "   theme_match  \n",
       "0        False  \n",
       "1         True  \n",
       "2        False  \n",
       "3         True  \n",
       "4         True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the evaluation function to all rows in the dataframe\n",
    "results = df_eval.join(df_eval.apply(evaluate_row, axis=1))\n",
    "\n",
    "# Save the evaluation results to a CSV file for further analysis\n",
    "results.to_csv(\"../data/evaluation/eval_results.csv\", index=False)\n",
    "\n",
    "# Display the first few rows of the results dataframe for inspection\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fecb7819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.67      1.00      0.80         4\n",
      "        fear       1.00      1.00      1.00        10\n",
      "         joy       0.89      1.00      0.94         8\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "     sadness       1.00      1.00      1.00        10\n",
      "       trust       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.78        41\n",
      "   macro avg       0.59      0.67      0.62        41\n",
      "weighted avg       0.73      0.78      0.75        41\n",
      "\n",
      "\n",
      "Theme classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Faith       0.58      0.50      0.54        14\n",
      "        Fear       0.74      0.93      0.82        15\n",
      " Forgiveness       0.00      0.00      0.00         5\n",
      "        Hope       0.33      0.33      0.33         3\n",
      "        Love       0.29      0.50      0.36         4\n",
      "\n",
      "    accuracy                           0.59        41\n",
      "   macro avg       0.39      0.45      0.41        41\n",
      "weighted avg       0.52      0.59      0.55        41\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\manue\\miniconda3\\envs\\LinguaAnimae\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Emotion classification report:\")\n",
    "print(classification_report(results[\"expected_emotion\"], results[\"pred_emotion\"]))\n",
    "\n",
    "print(\"\\nTheme classification report:\")\n",
    "print(classification_report(results[\"expected_theme\"], results[\"pred_theme\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbe2bbe",
   "metadata": {},
   "source": [
    "### 📋 Summary of Manual Evaluation (Section 6)\n",
    "\n",
    "This evaluation tested 42 manually curated examples with expected emotion and theme labels.\n",
    "\n",
    "#### 🧠 Emotion Classification\n",
    "- **Accuracy**: 78%\n",
    "- **Weighted F1-score**: 0.75\n",
    "- **Observations**:\n",
    "  - Excellent performance on `fear`, `joy`, and `sadness` (F1 > 0.94).\n",
    "  - `trust` was never predicted correctly (F1 = 0.00).\n",
    "  - Unexpected predictions for `neutral` suggest label filtering may be needed before evaluation.\n",
    "\n",
    "#### 🏷️ Theme Classification\n",
    "- **Accuracy**: 59%\n",
    "- **Weighted F1-score**: 0.55\n",
    "- **Observations**:\n",
    "  - Strong detection of `Fear` (F1 = 0.82), confirming model sensitivity to explicit emotional cues.\n",
    "  - Low recall for `Hope`, `Love`, and `Forgiveness`, possibly due to subtler context or limitations of zero-shot learning without fine-tuning.\n",
    "\n",
    "#### ✅ Conclusions\n",
    "- Emotion predictions are strong and usable in the MVP without additional training.\n",
    "- Theme classification is functional but limited. Only high-confidence themes (e.g. `Fear`, `Faith`) should be used in early recommendations.\n",
    "- Future improvements could involve:\n",
    "  - Refining prompts or context for theme detection\n",
    "  - Manual annotation + fine-tuning\n",
    "  - Filtering unexpected model outputs (e.g. `neutral`, `disgust`) for better evaluation\n",
    "\n",
    "This validation establishes a clear performance baseline for the MVP.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LinguaAnimae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
